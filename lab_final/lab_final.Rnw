\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\abovecaptionskip}{-5pt}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{url}

\begin{document}

\title{Lab Final - Stat215a - Stat 215A, Fall 2017}



\author{Yizhou Zhao}

\maketitle

\section{Introduction}
Understanding how the human brain functions remains one of greatest challenges in medical and psychological research. Medical tools such as functional Magnetic Resonance Imaging (fMRI), which is a class of imaging methods developed to demostrate the metabolism in the brain, gives tangible ways for scientist to analysis the mechanism of visual cognition of human-beings. Gallant lab collected the fMRI response from 1750 images. In this lab, I am trying to study what parts of our brain are active in visual processing and how they interact. 

\section{Exploratory Data Analysis}

\subsection{Data Background}
Gallant lab provided us with several data sets:
\begin{itemize}
\item \textbf{Response of brain voxels.} During the fMRI, scientists recored the response of 20 voxels from 1750 images. The response of each voxel is a continous variable, ranging from $-4.86$ to $4.78$.
\item \textbf{Image data.} Each one of the 1750 images is a $128 \times 128$ images with gray scale.   

<<Image and Wave, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 3, fig.width = 3, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

source("code/setup.R")
load("data/fMRIdata.RData")

#images = read.csv("data/fit_stim.csv",nrows = 5)
#load("data/fMRIdata.RData")
#sample1 = matrix(unlist(images[1,]),128,128)
#sample1 = t(sample1[128:1,])
#image(sample1,col = gray(1:128/129))

wsample1 = matrix(unlist(fit_feat[1,1:10816]),104,104)
wsample1 = wsample1[1:104,]
image(wsample1,col = gray(128:1/129))

@

\item \textbf{Wavelet Feature data.} Gabor wavelet pyramid transform the information of the image into frequency domain. The real part of 10921 Gabor wavelet is given and another non-linear transformation extracts the 10921 features from each image.

\item \textbf{Voxel locations.} The 3D coordinates of th 20 voxels in the brain.  

\end{itemize}

\subsection{Data visualization}

<<Voxel locations, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 5, fig.width = 5, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=


rownames(loc_dat) = 1:20
colnames(loc_dat) = c("x","y","z")
loc_dat = data.frame(loc_dat)
with(loc_dat, {
   s3d <- scatterplot3d(x, y, z,        # x y and z axis
                 color="blue", pch=19,        # filled blue circles
                 type="h",                    # vertical lines to the x-y plane
                 main="3D plot of voxels",
                 xlab="X",
                 ylab="Y",
                 zlab="Z")
    s3d.coords <- s3d$xyz.convert(x, y, z) # convert 3D coords to 2D projection
    text(s3d.coords$x, s3d.coords$y,             # x and y coordinates
         labels=row.names(loc_dat),               # text to plot
         cex=.5, pos=4)           # shrink text 50% and place to right of points)
})
@

<<Density of response, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 5, fig.width = 4, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

colnames(resp_dat) = paste0("V" ,as.character(1:20))
density_resp = melt(resp_dat,id = colnames(resp_dat))
colnames(density_resp) = c("id","voxel","response")

ggplot(density_resp, aes(response, ..density.., colour = voxel)) +
  geom_density() + ggtitle("Density plot for the response") + guides(fill=guide_legend(ncol=2))
@

Figure ??? shows the distributions of the responses for 20 voxels. All curves look similar from normal distribution and the differences between voxels are small. 

\section{Training and analysis}

\subsection{Methodology}
The experiment in Kay's ariticle upon fMRI based on mainly two stages. In the first stage, model estimation, fMRI data were recorded from visual areas while each subject viewed 1,750 natural images.Gabor wavelet pyramidnd described tuning along the dimensions of space, orientation and spatial frequency. In the second stage of Kay experiment, scientist tried to identify images from brain's response. I will only focus on the previous stage, to discover the relationship between the features of images and voxel responses. 

I decided before training these methods on a few approaches:

\begin{itemize}
\item \textbf{Individual lasso without feature screening}: In this approach, I will treat the response of every voxel individually with all the 10921 features included. 
\item \textbf{Individual lasso with feature screening}: As the same as the above method, I will treat the response of every voxel individually. However, I will apply some methods for dimentional reduction to get a pre-screening of the features. Specaifically, inspired by Fan's paper(2008)[???], Sure Independence Sampling will select $M$ features with the highest $M$ absolute correlation with responses. In my study, I select 1000 features from $fit_feat$.
\item \textbf{Multiple lasso regression}: This model treats all the response together as a matrix and preforms a multiple regression tast.

Also several regulization methods will be considered in our model including \textbf{AIC},\textbf{AICc},\textbf{BIC},\textbf{ESCV}[???] and \textbf{Mean square error}
\end{itemize}

\subsection{Lasso}
Lasso in the linear model trained with L1 penalty as regularizer. The optimization objective for Lasso is:
$$
\frac{1}{2n}||Y-X\beta||^2_2+\lambda||\beta||_1
$$
Lasso is able to achieve both of these goals by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, which forces certain coefficients to be set to zero, effectively choosing a simpler model that does not include those coefficients. 

\subsection{Feature Sceening}:
Fan and Lv (2008) introduced Sure Indepence Sceening for variable screening via independent correlation learning that tackles ultrahigh dimensional linear models. Sure Independence Sceening is a two-stage procedure.
\begin{itemize}
\item First filtering out the features that have weak marginal
correlation with the response, effectively reducing the dimensionality $p$ to a moderate scale below the sample size $n$. In my study, I select $N$ features with highest $N$ absolute correlation with reponses.
\item Then performing variable selection and parameter estimation
simultaneously through a lower dimensional penalized least squares method such as LASSO.
\end{itemize}

\subsection{Model selection}

<<IC plot, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 3.5, fig.width = 8, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

source("code/lasso.R")

train_x = fit_feat[1:1400,]
train_y = resp_dat[1:1400,]

test_x = fit_feat[1401:1750,]
test_y = resp_dat[1401:1750,]


CVtable = Get.IC.Table(train_x,train_y,1)

CVplot = melt(CVtable,id = "lambda")
#colnames(CVplot) = c("lambda","","value")
p1 = ggplot(data = CVplot,aes(x = lambda,y = value,colour = variable)) + geom_line() + ylim(c(-250,500)) + theme_bw()+theme(legend.justification=c(1,1),legend.position=c(1,1),legend.title=element_blank())
  

cv.lasso.1 = cv.glmnet(train_x,train_y[,1],type.measure = "mse",nfolds = 10,nlambda = 100)

p2 = ggplot(data = data.frame(lambda = cv.lasso.1$lambda,
        MSE = cv.lasso.1$cvm), aes(x = lambda,y = MSE)) + geom_line()+
  theme_bw()



ESCV = read.csv("data/output/ESCV1.csv")
p3 = ggplot(data = data.frame(lambda = CVtable$lambda[50:1],
        ESCV = ESCV$x), aes(x = lambda,y = ESCV)) + geom_line() + theme_bw()

grid.arrange(p1,p2,p3,ncol = 3)
@
The above??? figure??? gives an example of applying different model selection criteria.  BIC shows a different pattern and it keeps going down when $\lambda$ increases. I can choose BIC criterion in this case because it does not provide me with any meaningful $\lambda$. The trends of AIC, AICc, MSE and ESCV are similar to each other, with the $\lambda$ minimizing those criteria close to $0.06$. It is worth mentioning that ESCV has a slight different pattern: dropping really fast to zero and then slowly going up as $\lambda$ increases. Here I choose the traditional $MSE$ (comes from cross-valiation) as model selection criterion for the rest of analysis, for it is cost least for calculation.   


<<Correlation plot, echo = FALSE, message=FALSE, warning=FALSE, fig.height = 4, fig.width = 5, fig.align='center', fig.pos='h', fig.cap="Histogram of the correlation similarity measure for different values of k">>=

lasso_cor = read.csv("data/output/lasso_cor.csv")
multiple_lasso_cor = read.csv("data/output/multiple_lasso_cor.csv")
screen_lasso_cor = read.csv("data/output/screen_lasso_cor.csv")

cor_table = data.frame(voxel = 1:20,lasso_cor,multiple_lasso_cor,screen_lasso_cor)
plot_table = melt(cor_table,id = "voxel")
colnames(plot_table) = c("voxel","type","correlation")


ggplot(data = plot_table,aes(x = voxel,y = correlation,colour = type))+
  geom_point() + geom_line(size = 0.5)

@
In my model, I divide my data into 1400 samples for training and validation, and the rest 350 for testing. The cross-validation process for LASSO has 100 different values for hyper parameter $\lambda$, and each validation has ten folds.

\subsection{Correlation and diagnosis}

Figure ??? shows the correlations between predicted and real responses for each one of the 20 voxels. Generally, multiple LASSO performs slightly better than individual LASSO. We can also read from the figure that the range of correlations is from near 0 to the highest 0.6. The correlations for voxels 10,13,16,20 are the worst. Three different LASSO methods all show poor results for those voxels. This is probably because of locations of those voxels. 3D plot of the locations tells that voxel 10,13,16,20 all locate at the border, which may be the indiction to try some non-linear models for those voxels.


\subsection{Intepretation}
This part is for the intepretation for the best LASSO method--the multiple regression. In details, after trying $\lambda$ for cross-validation and choosing the one with least mean square loss, I get the model with $120$ features(including one constant term.)

\end{document}