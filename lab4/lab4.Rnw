\documentclass{article}

\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\abovecaptionskip}{-5pt}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{url}
\usepackage{bm}

\begin{document}

\title{Lab 4 - Cloud detection - Stat 215A, Fall 2017}

\author{Olivia Angiuli, Miyabi Ishihara, Yizhou Zhao}

\maketitle

\section{Introduction}

In 1999, NASA's Multiangle Imaging SpectroRadiometer (MISR) data generated a massive data set of Arctic images that help provide insight into recent global changes of ice coverage due to increasing surface air temperatures.  The dataset is rich, generated by a satellite that collected images along 233 different geographical paths, with 180 blocks in each path, providing images of any given region every 16 days.

One of the biggest statistical challenges of this dataset is to conclude, for each image, which regions are ice-covered, as opposed to snow-covered or simply unknown.  Using only a small subset (three images) of this data, each of which produced radiances from 5 different angles, we use expert labels in order to train and evaluate classification models that can help differentiate between icy versus cloudy surfaces.

<<setup, echo = FALSE, message=FALSE, warning = FALSE, fig.height = 5, fig.width = 5, fig.align = 'center', fig.pos = 'h', fig.cap = "Histogram of the correlation similarity measure for different values of k">>=

library(tidyverse)
library(gridExtra)

source("R/explore.R")

# Get the data for three images
path <- "data"
image1 <- read.table(paste0('data/', 'image1.txt'), header = F)
image2 <- read.table(paste0('data/', 'image2.txt'), header = F)
image3 <- read.table(paste0('data/', 'image3.txt'), header = F)

# Add informative column names.
collabs <- c('y', 'x', 'label', 'NDAI', 'SD', 'CORR', 'DF', 'CF', 
             'BF', 'AF', 'AN')
names(image1) <- collabs
names(image2) <- collabs
names(image3) <- collabs
@

\section{Exploratory Data Anaysis}

\subsection{Data background}

We are provided with three images, each of which provide the following information:

\begin{itemize}
\item \textbf{Expert labelling. } Experts visually inspected each image, classifying each pixel as cloudy (+1), not cloudy (-1), or unlabelled (0).  These are the labels with which we will train our supervised algorithms, and with which we will test our results.
\item \textbf{Radiances from five angles. } We were given satellite images for three different 275m x 275m regions, each from five different angles as shown below (where ``F" indicates the forward direction of flight and ``A" represents the rear direction):

\begin{figure}
\begin{center}
\includegraphics[width=0.35\textwidth]{extra/reference_angles}
\end{center}
\caption{The five angles from which we have radiance measures, for each of the three images.}
\end{figure}

As will be described in the next section, differences in radiance measurements of the same image from different angles can tell us about the presence or absence of clouds and/or ice.
\item \textbf{Feature values. } Shi et. al. developed three features that capture spatial information that helps distinguish between ice and clouds:
\begin{itemize}
\item \textbf{CORR - }\emph{an average linear correlation of radiation measurements at different view angles}.\\
A cloud may obstruct the view of underlying ice from some angles but not others.  Figure 1, above, illustrates a scenario in which DF provides an unobstructed view of the underlying image that the other angles cannot provide.\\
This suggests that low CORR images often suggest clouds: however, low-altitude clouds or smooth cloud-free areas may break this trend, requiring the following two features.
\item \textbf{SD - }\emph{standard deviation within groups of MISR}.\\
SD helps identify smooth surfaces (areas with low standard deviations), as well as to identify a baseline for background measurement error that can be filtered out in the model.
\item \textbf{NDAI - }\emph{a proxy for surface roughness, measured by the normalized difference between measurements in the forward versus backward pointing cameras}.\\
The intuition is that a low-altitude cloud has more roughness than an ice- or snow-covered surface.  This feature combines with CORR to help differentiate between low CORR images that have low-altitude clouds versus are cloud-free.
\end{itemize}
\end{itemize}

\subsection{Data visualization}

The following plots visualize the three provided images.  The first set of images shows the raw data from the perspective of the AN-camera.  The second set of images shows the same three images, classified by whether an expert determined that a given area was ice (pink), unknown (green), or cloudy (blue).

<<plot-raw-data, cached = TRUE, echo = FALSE, message = FALSE, warning = FALSE, dev = 'png', dpi = 300, fig.height = 2, fig.width = 6, fig.align = 'center', fig.pos = 'h', fig.cap = "Raw (from AN-camera) image data for the three provided images.">>=

# Raw images
grid_arrange_shared_legend(plotRawImage(image1, "AN") +
    ggtitle("Raw satellite data of snow/ice,from AN-camera"),
    plotRawImage(image2, "AN") + ggtitle(""),
    plotRawImage(image3, "AN") + ggtitle(""),
    ncol = 3, nrow = 1, position = "right")
@

<<plot-expert-labelled-data, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Expert-labelled image data for the three provided images.">>=

# Expert-labelled images
grid_arrange_shared_legend(plotClasses(image1) +
                           ggtitle("Expert-labelled satellite data
                                   of snow/ice"),
                           plotClasses(image2) + ggtitle(""),
                           plotClasses(image3) + ggtitle(""),
                           ncol = 3, nrow = 1, position = "right")
@

\subsection{Do different angles provide different information?}

Yes!  Below, we show image 1 from all 5 different angles that we are provided in the data set: from left to right, 70.5$^{\circ}$ (DF), 60.0$^{\circ}$ (CF), 45.6$^{\circ}$ (BF), 26.1$^{\circ}$ (AF) in the forward directions,  and 0$^{\circ}$ (AN).

One can spot higher resolution in the bottom left corner (which corresponds to mostly cloudy and/or unknown regions) as the angles becoming increasingly acute.  This supports the reasoning for Shi et. al.'s construction of the ``CORR" and ``NDAI" features: cloudy regions might have more variation in radiances between different angles.

<<plot-images-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Image 1, by increasingly acute angles.">>=

grid_arrange_shared_legend(plotRawImage(image1, "DF") +
    ggtitle("Image 1, taken from increasingly acute angles"),
    plotRawImage(image1, "CF") + no_axis + ggtitle(""),
    plotRawImage(image1, "BF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AF") + no_axis + ggtitle(""),
    plotRawImage(image1, "AN") + no_axis + ggtitle(""),
    ncol = 5, nrow = 1, position = "right")
@

Plotting the conditional densities of cloud- versus ice-covered regions confirms that different angles do indeed help differentiate between cloud and ice.  We notice two trends in particular.  First, icy regions tend to occur at a resolution of around 275.  This can effectively set a high ``prior" for iciness for pixels with a similar resolution.  Second, we notice that cloudy regions have a much wider range of resolutions that change with the angle of measurement.  Again, this confirms Shi et. al.'s intuition that differences between measurements from different angles often correspond to cloudiness.

<<plot-densities-by-angle, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "DF") +
    ggtitle("Image 1 conditional densities, by measurement angles") +
    no_axis + xlab("DF"),
    plot_conditional_densities(image1, "CF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "BF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AF") + no_axis + ggtitle(""),
    plot_conditional_densities(image1, "AN") + no_axis + ggtitle(""),
    ncol = 5, nrow = 1, position = "right")

@

\subsection{How do features help distinguish between cloud and ice?}

Recall that the three features, as described in detail above, are:
\begin{itemize}
\item CORR (correlation of images from different view angles)
\item SD (the standard deviation within groups of MISR)
\item NDAI (a proxy for surface roughness, measured by the normalized difference between measurements in the forward vs. backward pointing cameras)
\end{itemize}

By itself, NDAI has the most discriminatory power between the presence of ice and the absence of ice (either cloudy or unknown).  This can be seen by the distinct cloud vs ice peaks in the leftmost graph below.

<<plot-features-for-image-1, cached=TRUE, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Conditional densities of ice vs. cloud vs. unknown by different angles. Cloud resolutions become increasingly bimodal as measurement angle decreases.">>=

grid_arrange_shared_legend(plot_conditional_densities(image1, "NDAI") +
    ggtitle("Image 1 NDAI"),
    plot_conditional_densities(image1, "CORR") + ggtitle("Image 1 CORR"),
    plot_conditional_densities(image1, "SD") + ggtitle("Image 1 SD"),
    ncol = 3, nrow = 1, position = "right")
@

The discriminatory power of NDAI can also be seen from the following side-by-side plot of NDAI resolution values versus the true classes of image 1.  Note that higher values of NDAI tend to correspond to ``no ice" regions.

<<plot-ndai-values, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

grid.arrange(plotRawImage(image1, "NDAI") + ggtitle("Image 1 NDAI"),
    plotClasses(image1) + ggtitle("True classes"), ncol = 2, nrow = 1)

@

\newpage

Shi et. al. also discuss how NDAI and CORR, together, provide even more information than the two features alone: they hypothesize that high CORR + low NDAI images would more strongly suggest ice than high CORR alone.  This relationship, though, is not readily seen in the following pairwise feature plots, largely because there are few high CORR pixels that correspond to ice-covered regions.

<<ndai-vs-corr, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="High NDAI values tend to correspond to unknown or cloudy regions; low NDAI values tend to correspond to icy regions.">>=

# plot all pairwise features, across all three image datasets
grid_arrange_shared_legend(
  plot_feature_vs_feature("NDAI", "CORR") + 
    ggtitle("Pairwise feature scatterplots"),
  plot_feature_vs_feature("NDAI", "SD") + ggtitle(""),
  plot_feature_vs_feature("CORR", "SD") + ggtitle(""))

@

\section{Classifying ice- versus snow-covered surfaces}

The classification approach of Shi et. al. hinges upon an enhanced linear correlation matching (ELCM) algorithm.  However, we noted that this approach depended on having data images available from multiple orbits and/or consecutive MISR blocks.  Since we do not have access to this full data set, reproducing this ELCM approach is not possible.

For that reason, we instead resort to more classic classification methods: quadratic and linear discriminant analysis, logistic regression, and random forests.  First, we train baseline versions of these methods.   We note that random forests produces the best AUC values of the three baseline models, so we proceed to do additional parameter tuning and analysis for that model.

We decided before training these methods on a few approaches:

\begin{itemize}
\item \textbf{Three-fold cross validation. } Since labels of adjacent pixels in an image are not truly independent, we decided to perform three-fold cross validation: that is, we would train three different versions of each model, each time setting aside an entire image for testing purposes.\\
This is much preferred over randomly sampling points, since regions of ice and cloud often span many adjacent pixels, meaning that training on one of these pixels would enhance the test performance of the pixels nearby to it.
\item \textbf{Training only on positive- and negative- expert labels. } We do not train our model on data points whose expert classification was a 0 (unknown): we would only like our model to learn classification based on points whose true value is known.
\end{itemize}

\section{LDA/QDA}

<<fit-LDA-QDA-part, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curves for LDA/QDA three-fold cross-validation with AUC scores.">>=

source("R/lda-qda.R")

training.image <- list(image1,image2,image3)

#Only to train and testing on the labeled data
Qimage1 = mutate(image1, Image = 1) %>% filter(label != 0)
Qimage2 = mutate(image2, Image = 2) %>% filter(label != 0)
Qimage3 = mutate(image3, Image = 3) %>% filter(label != 0)

#the plotting information for ROC curves.
plot.info <- qda.lda.info(Qimage1, Qimage2, Qimage3)

grid_arrange_shared_legend(plot.info[[2]], plot.info[[1]],
                           ncol =2, nrow = 1, position = "right")
@

Quadratic discriminant analysis (QDA) is a method used in order to fit the best separator between two different classes of data. It assumes that  each class $i$ has a Gaussian distribution with mean $\mu_i$ and covariance matrix $\Sigma_i$, that is:

$$
p(y = i | x) = \frac{\pi_i|2\pi\Sigma_i|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)]}{\sum_k\pi_k|2\pi\Sigma_k|^{-\frac{1}{2}}\exp[-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)]}
$$

Linear discriminant analysis (LDA) is nearly identical to QDA, except that it makes the further assumption that the covariance matrix is the same across all classes. We note from our exploratory analysis (e.g. Figure 6) that the assumption of Gaussian data within each class is reasonable, albeit not perfect.  
We also note that Figure 9 shows the ROC curves for the leave-one-image-out cross validation results from our LDA and QDA models.  Note that QDA performs slightly better than LDA in the first and second fold and worse in the third fold, where both QDA and LDA performed significantly worse than normal. This may suggest that image 1 and image 2 are similar, 
so training on one of them helps predict the classes for the other, but that image 3 might have feature combinations that are significantly different from those found in image 1 and image 2.

<<fit-qda-3, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Plots showing correctly-classified and misclassified points using the LDA model.">>=

conf.info <- qda.confustion(Qimage1, Qimage2, Qimage3)

grid_arrange_shared_legend(conf.info[[1]], conf.info[[2]], conf.info[[3]],
                           ncol = 3, nrow = 1, position = "right")
@

Figure 10 shows the confusion plot after selecting a cloud-vs.-ice threshold of 0.21, which was determined by averaging the thresholds in the three-fold validation to make the point $(TRR, FPR)$ furthest away from the line $y = x$.

As the above plot shows, misclassification tends to occur in entire regions rather than distinct points.  False positive rates in the three folds respectively are 0.121, 0.122, and 0.195, and false negative rates are 0.018, 0.001, and 0.165. The third fold cross-validation, in particular, has a high false negative rate because image 3 has a small area of cloud in the upper left part of the image that is largely misclassified.

We deduce from this confusion plot that the main problem with the QDA/LDA classifier is its inability to judge whether the "dark" regions in the images are cloud versus ice.  This pattern is consistent between the middle lower left region in image one, the lower right region in image two, and the lower middle and lower right regions in image three.

\subsection{Logistic regression}

Logistic regression models the probability of the response taking a particular value based on the covariates. In this case, we model the probability of a pixel $Y_i$ being cloudy, given the three features: 

$$
\pi_i = P(Y_i = 1|\bm{X_i = x_i}) = \frac{exp^(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)}{1 + exp^(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)}, 
$$

or 

$$
log\big(\frac{\pi_i}{1-\pi_i}\big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3, 
$$

where $\beta_1, \beta_2, \beta_3$ correspond to the estimates of the coefficients for our three features.\\

Unlike in LDA or QDA, logistic regression does not make assumption on the normality of features. It also does not assume a linear relationship between the outcome and covariates or homoscedasticity. However, logistic regression does require that $Y_1, Y_2, ..., Y_n$ be independently distributed, which is not satisfied in this case; Pixels have spatial correlation in that those that are close to each other are more likely to have the same binary outcome. \\ 

Partly for this reason, classification accuracy of logistic regression is lower than those of discriminant analysis and random forest (Figure 11). Also, unlike in DA and random forest, prediction on image 3 is not substantially worse than predictions on images 1 and 2. Logistic regression is less sensitive to outliers, and this may explain why prediction on image 3 is not the worst, even though it has different feature combinations from those of images 1 and 2. 

<<fit-logistic-regression, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for logistic regression three-fold cross-validation.">>=

source("R/logistic_regression.R")

prediction_LS1 <- trainAndEvaluateLR(train_data_23, images1_filtered)
prediction_LS2 <- trainAndEvaluateLR(train_data_13, images2_filtered)
prediction_LS3 <- trainAndEvaluateLR(train_data_12, images3_filtered)

# calculate false positive and true positive rates into dataframe
fpr_tpr_rates_df <- data.frame(
  rbind(calculateROCValues(prediction_LS3, 3),
      calculateROCValues(prediction_LS2, 2),
      calculateROCValues(prediction_LS1, 1)))
colnames(fpr_tpr_rates_df) <- c("fpr", "tpr", "image_num")


# plot ROC for three images 
ggplot(fpr_tpr_rates_df) + 
  geom_line(aes(x = fpr, y = tpr, color = factor(image_num))) + 
  scale_color_discrete(name = "Predictions on \nImage Number",
                    labels = c("1 (AUC = 0.897)", "2 (AUC = 0.953)", 
                               "3 (AUC = 0.898)")) + 
  theme_bw() + 
  labs(x = "False positive rate",
         y = "True positive rate",
         title = "Logistic Regression ROC curve: Leave-one-out CV") + 
  theme(panel.border = element_blank(), 
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9),
        plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 9))

@


\subsection{Random forests}

Random forests are an ensemble learning method that combine the prediction of multiple component decision trees in order to make a probabilistic determination of the class of an input.

Random forests are ``random" in a few different steps:
\begin{itemize}
\item Tree bagging. Each component tree randomly selects, via bootstrapping, a training subset from the total training set.
\item Feature bagging. At each candidate split of a component tree, a random subset of the features are used.
\end{itemize}

This is illustrated visually in the below picture.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{extra/rf_explanation.jpg}
\end{center}
\caption{Graphical introduction to random forests [2].}
\end{figure}

In our case, we fit random forests with 100 trees using three-fold cross validation approach described above.  The resulting ROC-curve is shown below, and reveals strong performance, shown by the model's ability to achieve a high true positive rate with quite a low corresponding false negative rate.

<<fit-random-forest, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="ROC curve for random forest three-fold cross-validation.">>=

source("R/random_forest.R")

# generated predictions for each of the three-fold CV data sets
prediction3 = trainAndEvaluateRF(train_data_12, image3_filtered, 100)
prediction2 = trainAndEvaluateRF(train_data_13, image2_filtered, 100)
prediction1 = trainAndEvaluateRF(train_data_23, image1_filtered, 100)

# calculate false positive and true positive rates into dataframe
fpr_tpr_rates_df <- data.frame(
  rbind(calculateROCValues(prediction3, 3),
      calculateROCValues(prediction2, 2),
      calculateROCValues(prediction1, 1)))
colnames(fpr_tpr_rates_df) <- c("fpr", "tpr", "image_num")

# plot results
ggplot(fpr_tpr_rates_df) +
  geom_line(aes(x = fpr, y = tpr, color = factor(image_num))) +
  labs(x = "False positive rate",
    y = "True positive rate",
    title = "Random Forest ROC curve: Leave-one-out CV") +
  scale_color_discrete(name = "Predictions on Image Num",
                      labels = c("1 (AUC=0.956)",
                                 "2 (AUC=0.965)",
                                 "3 (AUC=0.885)")) +
  # clean up plot a little bit
  theme_bw() +
  # make the title and axes labels smaller
  theme(panel.border = element_blank(),
        legend.position = "right",
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9),
        plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 9))

@

\subsubsection{Parameter tuning}

The three main parameters to tune are the number of trees in the model, the maximum number of leaves (nodes) in each tree, and the number of features to use in each tree.

% Note that parameter tuning takes a long time, so we didn't want it to
% run every time the report compiled.  You can view the code in the
% parameter_tuning_rf.R file.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{extra/parameter_tuning_graphs}
\end{center}
\caption{The above graphs show the marginal effect between AUC and changing three parameters of a random forest (number of trees, maximum unmber of nodes, and number of features to try in each tree).}
\end{figure}

First, we examine the marginal behavior between AUC and each parameter.  We see from the below graph that AUC is maximized with:
\begin{itemize}
\item \textbf{A high number of trees}. This is intuitive because more trees lead to a more stable prediction.  Since each tree uses different baseline data and different features, overfitting is usually not a big concern.
\item \textbf{A moderate number of maxnodes}. This suggests that having too many leaves in any given component tree may lead to overfitting.
\item \textbf{A small number of features tried in each tree.}  At first, this may be counterintuitive: you would think if more features are considered within each tree, this would lead to a strictly better result.  However, since our features may be correlated, then a small number of features may be better for prediction since it may decouple the correlation between features and avoid overfitting.
\end{itemize}

The below table shows the three best and three worst parameter combinations, in terms of AUC performance. The best parameter combination coresponds to 500 trees with a maximum of 10 nodes per tree and 1 feature tried at each point, which outputs a model with an AUC of 0.900.

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Num Trees & Max Nodes & Num Features & AUC \\ \hline
    500 & 10 & 1 & 0.900 \\ \hline
    100 & 10 & 1 & 0.898 \\ \hline
    50 & 10 & 1 & 0.896 \\ \hline
    ... & ... & ... & ... \\ \hline
    100 & 10 & 3 & 0.810 \\ \hline
    50 & 10 & 3 & 0.809 \\ \hline
    10 & 10 & 3 & 0.803 \\ \hline
    \hline
    \end{tabular}
\end{center}



\subsubsection{Systematic misclassification errors}

In the three images, at least 80\% of the pixels are correctly classified as cloudy or clear. Prediction made on image 2 has the highest accuracy with very low false negative rate, whereas the prediction made on image 3 has the lowest accuracy rate. 

\begin{table}[h]
\centering
%\caption{Misclassification rate}
%\label{misclassification_rate}
\begin{tabular}{l|l|l|ll}
\cline{1-4}
 & correctly classified (\%) & false positive (\%) & false negative (\%) &  \\ \cline{1-4}
image 1 & 89.0 & 4.8 & 6.2 &  \\
image 2 & 93.1 & 5.6 & 1.3 &  \\
image 3 & 80.4 & 8.6 & 11.0 &  \\
all images & 88.4 & 6.2 & 5.5 &  \\ \cline{1-4}
\end{tabular}
\end{table}

There are certain feature values at which misclassification occurs more frequently than others. For example, false negatives tend to occur in the upper extreme and lower values of NDAI. It makes sense conceptually that low NDAI values are mistakenly classified as clear, because clear regions have more isotropic surface-leaving radiation. False positives tend to occur in the upper half of NDAI values. This is in part explained by the presence of rough ice regions, which may have been mistakenly classified as clouds because they have moderately high NDAI values. \\

NDAI, which differs substantially between cloud and clear pixels, is the most indicative of how a given pixel is misclassified. On the other hand, SD is the least informative feature, because both correct and false classification occurs at almost every value of SD. \\

Pixels are rarely misclassified at small values of NDAI, or at large values of CORR.

<<misclassification_plot, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Patterns in misclassification errors by different feature values. Grey points denote pixels that are correctly classified, red denotes clear pixels  incorrectly classified as cloud, and blue denotes cloudy pixels incorrectly classified as clear.">>=

# for each image, create data that has both predicted and actual labels
data1 <- cbind(prediction1, image1_filtered)
data2 <- cbind(prediction2, image2_filtered)
data3 <- cbind(prediction3, image3_filtered)
data <- rbind(data1, data2, data3)

# set a threshold
threshold <- 0.5

# create a categorical variable that indicates whether a pixels is correctly classified or not correctly classified
data$class <- NA
data$class[data$predicted >
             threshold & data$true == 1] <- "correctly classified"
data$class[data$predicted <=
             threshold & data$true == 0] <- "correctly classified"
data$class[data$predicted > threshold & data$true == 0] <- "false positive"
data$class[data$predicted <= threshold & data$true == 1] <- "false negative"
# convert the variable as factor
data$class <- factor(data$class, 
                     levels = c("correctly classified", 
                                "false positive", "false negative"))

# plot NDAI vs SD with colors indicating misclassification type
g1 <- 
  ggplot() + 
  geom_point(data = data, aes(x = NDAI, y = SD, color = class), alpha = 0.1) + 
  scale_color_manual(values = c("gray70", "deeppink", "turquoise2")) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  theme_bw() + 
  theme(legend.position = "none", 
        panel.border = element_blank(), 
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 9))

# plot NDAI vs CORR 
g2 <- 
  ggplot() + 
  geom_point(data = data, aes(x = NDAI, y = CORR,
                              color = class), alpha = 0.1) + 
  scale_color_manual(name = "",
                     values = c("gray70", "deeppink", "turquoise2")) +
  theme_bw() + 
  theme(legend.position = "none", 
        panel.border = element_blank(), 
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 9))

# plot SD vs CORR 
g3 <- 
  ggplot() + 
  geom_point(data = data, aes(x = SD, y = CORR, color = class), alpha = 0.1) + 
  scale_color_manual(name = "", values = c("gray70", "deeppink",
                                           "turquoise2")) +
  theme_bw() + 
  theme(legend.position = "none", 
        panel.border = element_blank(), 
        axis.title.x = element_text(size = 9),
        axis.title.y = element_text(size = 9),
        axis.text = element_text(size = 9))

grid_arrange_shared_legend(g1, g2, g3, ncol = 3, nrow = 1,
                           position = "bottom")

@
<<Confusion-Plot, echo = FALSE, message=FALSE, warning=FALSE, dev='png', dpi=300, fig.height = 2.5, fig.width = 6, fig.align='center', fig.pos='h', fig.cap="Plots showing correctly-classified and misclassified points using the random forest model.">>=
#Get the confusion plots
data1 = data1 %>% mutate(confusion = ifelse(label == -1,
   ifelse(predicted< threshold, "TN","FP"),
   ifelse(predicted >= threshold, "TP", "FN")))
         
data2 = data2 %>% mutate(confusion = ifelse(label == -1,
  ifelse(predicted< threshold, "TN", "FP"),
  ifelse(predicted >= threshold, "TP", "FN")))

data3 = data3 %>% mutate(confusion = ifelse(label == -1, ifelse(
  predicted< threshold, "TN", "FP"),
  ifelse(predicted >= threshold, "TP", "FN")))

ggconf <- list(geom_point(aes(x = x, y = y,
                              group = confusion,
                              colour = confusion), alpha = 0.1),
                 scale_colour_manual(values = c("TP" = "gray70",
                                                "FN" = "turquoise2",
                                                "TN" = "lightgoldenrod1",
                                                "FP" = "deeppink",
                                                "Unknown" = "#777777")),
               theme(legend.position = "right", aspect.ratio = 1),
               guides(color = guide_legend(override.aes =
                                           list(size = 2.5, alpha = 0.5))),
               theme_bw(), no_axis)
  

# Plot the three images
rf1.conf <- ggplot(data1) + ggconf +
  ggtitle("Image 1") + blank_theme + no_axis +
  theme(axis.title.x = element_blank())

rf2.conf <- ggplot(data2) + ggconf +
  ggtitle("Image 2") + blank_theme + no_axis +
  theme(axis.title.x = element_blank())

rf3.conf <- ggplot(data3) + ggconf + 
  ggtitle("Image 3") + blank_theme + no_axis +
  theme(axis.title.x = element_blank())

grid_arrange_shared_legend(rf1.conf, rf2.conf, rf3.conf,
                           ncol = 3, nrow = 1,
                           position = "right")

@
The confusion plot of random forest shows some different patterns compared to LDA/QDA and logistic regression. First, random forest seems to be suffer less from high false positive rate shown in the plot. For example, the island like southwest clear part in image one, the southeast parts in image two and image three which are almost completely misclassified in LDA/QDA model, have some area at the border of it is classified correctly in random forest. Besides, classification from random forest seems to be of more noise, which means that there are some dots insides a large cloudy or clear area. 

\subsubsection{Generalizability of the model}

Ultimately, a model learns best about the subspaces that it has trained on, and a random forest model is no different.  Decision criteria within every component decision tree of a random forest are chosen to be optimal for data whose ranges are similar to those of the training examples.\\

Although we don't have information about the variability of other images within the feature space of NDAI, CORR, and SD, we can reasonably assume that \textbf{the model will perform fairly well in images whose feature combinations are similar to those seen in Images 1, 2, and 3}, and will perform less accurately for images whose feature combinations are not.

\section{Conclusion}

In this report, we have explored and modeled various classification methods to perform cloud detection, based on satellite images of radiances in the polar regions. Data consited of radiation values of three locations, each measured from six different angles. Classification models were trained on two images using three useful features that can differentiate pixels from cloudy ones, and the models were tested on the remaining single image. Validity of each model was assessed via three-fold cross validation technique and the area under the Receiver Operation Curves (ROC). \\

Among four probability prediction models -- logistic regression, linear discriminant analysis, fisher discriminant analysis, and random forest -- random forest provided the most accurate result. Misclassification analysis lead us to identify certain subspaces and feature values that are more likely to be misclassfied than others, such as rough ice regions that were incorrectly classified as clouds. Performance of the model is likely to depend on how similar the test image is to Images 1, 2, and 3, in terms of feature combinations. 

\section{References}

(1)  Shi, Tao, et al. "Daytime arctic cloud detection based on multi-angle satellite data with case studies." Journal of the American Statistical Association 103.482 (2008): 584-593.\\
(2) Random Forest Interactive Discussion with Example. \url{https://www.youtube.com/watch?v=ajTc5y3OqSQ}

\end{document}
